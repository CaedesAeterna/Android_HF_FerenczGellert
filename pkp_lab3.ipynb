{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Labor 3\n",
        "\n",
        "A párhuzamos algoritmusok célja a [teljesítmény növelése](https://courses.cs.washington.edu/courses/csep524/07sp/poppChaper1.pdf). Ezt azzal érik el, hogy a számításokat egyszerre több számítógépen vagy számítási egységen végzik el, így csökkentve a számítási időt, vagy lehetővé téve pontosabb eredmények elérését egy adott időegység alatt.\n",
        "\n",
        "\n",
        "# Teljesítménymértékek\n",
        "\n",
        "A teljesítménynövekedés számszerű kifejezése érdekében áttekintjük, hogy milyen módszerek állnak rendelkezésre.\n",
        "\n",
        "## Gyorsítás (speedup)\n",
        "\n",
        "Az időarány, amelyben a párhuzamos algoritmus végrehajtása gyorsabb, mint a szekvenciális algoritmusé. A gyorsítás értéke az eredeti, szekvenciális idő és az új idő hányadosa.\n",
        "\n",
        "Jelölje $T(n,p)$ azt a függvényt, ami megadja, hogy egy párhuzamos program egy $n$ méretű feladatot mennyi idő alatt old meg $p$ processzor használatával.\n",
        "\n",
        "Adott $n$-méretű feladatra a $T(n,p)$ párhuzamos $p$ processzoron futó program reális  **gyorsítását (gyorsítási faktort)**\n",
        "\n",
        "$S(n,p)=\\frac{T^*(n)}{T(n,p)}$\n",
        "\n",
        "adja meg, ahol $T^*(n)$ a leggyorsabb ismert soros algoritmus futási ideje.\n",
        "\n",
        "\n",
        "\n",
        "A környezettől függően a gyorsítás vonatkozhat $p$ folyamat vagy $p$ szál alkalmazására is.\n",
        "\n",
        "$T^*(n)$ nem mindig ismert, vagy nem implementálható, ezért gyakran a relatív **_gyorsítását_**  használjuk, ahol a párhuzamos program egy processzoros futási idejét vesszük figyelembe:\n",
        "\n",
        "$S(n,p)=\\frac{T(n,1)}{T(n,p)}$\n",
        "\n",
        "\n",
        "## Hatékonyság (efficiency)\n",
        "\n",
        "Másik fontos metrika a **_párhuzamos hatékonyság_** ami kifejezi a gyorsítás arányát a felhasznált számítási erőforrásokhoz képest. Az hatékonyság értéke a gyorsulás és a felhasznált erőforrások (például a processzorok vagy a magok száma) hányadosa:\n",
        "\n",
        "$E(n,p)=\\frac{S(n,p)}{p}=\\frac{T^*(n)}{p*T(n,p)}$\n",
        "\n",
        "Ez a mutató betekintést nyújt a processzorok kihasználtságára, ideális esetben ez az érték 1 (lineáris gyorsulás).\n",
        "\n",
        "Egy párhuzamos programot akkor nevezünk **_skálázhatónak_**, ha a párhuzamos hatékonyság fenntartható. Skálázhatóság (scalability): a rendszer képessége, hogy a számítási feladatokat nagyobb erőforrásokkal bővítve hatékonyan kezelje.\n",
        "\n"
      ],
      "metadata": {
        "id": "ikVGiziWBV53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Időmérésa CUDA események segítségével\n",
        "\n",
        "A CUDA esemény ([CUDA event](https://docs.nvidia.com/cuda/cuda-runtime-api/index.html#group__CUDART__EVENT)) egy szinkronizációs jel, amely lehetővé teszi a programozó számára, hogy monitorizálja a CUDA munkafolyamatokat, szinkronizálja a CUDA adatfolyamokat (CUDA streams) és lehetőve teszi a [CUDA kernelek pontos időmérését](https://developer.nvidia.com/blog/how-implement-performance-metrics-cuda-cc/).\n",
        "\n",
        "\n",
        "A `cudaEvent` egy CUDA API függvény, amely lehetővé teszi a programozók számára, hogy mérjék a CUDA műveletek végrehajtási idejét. A `cudaEvent` funkciók segítségével időbélyegeket állíthatunk be, majd ezen bélyegek között eltelt időt mérhetjük.\n",
        "\n",
        "A következő lépésekkel használjuk a `cudaEvent`-et időmérésre:\n",
        "\n",
        "1. Létre kell hozni két `cudaEvent` objektumot, amelyek közül az egyiket a művelet kezdeteként, a másikat pedig a művelet befejezéseként jelöljük meg. Például:\n",
        "  ```cpp\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "  ```\n",
        "2. A [`cudaEventRecord`](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html#group__CUDART__EVENT_1gf4fcb74343aa689f4159791967868446) függvény segítségével be kell állítanunk a start és stop bélyegeket a művelet elkezdése elött és a befejezésnél. Például:\n",
        "  ```cpp\n",
        "  cudaEventRecord(start, 0);\n",
        "  // CUDA műveletek végrehajtása\n",
        "  // pl. cudaMalloc, cudaMemcpy, kernel hívás, eredmények visszamásolása, memória felszabadítás\n",
        "  cudaEventRecord(stop, 0);\n",
        "  ```\n",
        "  A második paraméter a `cudaEventRecord` hívásokban az adatfolyamot ([CUDA stream](https://developer.download.nvidia.com/CUDA/training/StreamsAndConcurrencyWebinar.pdf)) jelöli, és a 0 érték azt jelzi, hogy az esemény a null adatfolyamhoz van rendelve.\n",
        "\n",
        "  Az adatfolyamok (stream) olyan CUDA fogalmak, amelyek lehetővé teszik a kernel futtatását és az adatmozgatást aszinkron módon. Az adatfolyamok segítségével párhuzamosíthatjuk a műveleteket és jobban kihasználhatjuk a hardverünk párhuzamosítási képességeit.\n",
        "\n",
        "3. A [`cudaEventSynchronize`](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html#group__CUDART__EVENT_1g949aa42b30ae9e622f6ba0787129ff22) függvény használatával megvárjuk, amíg a stop időbélyeg rögzítése megtörténik, majd ellenőrizzük az esetleges hibákat. Például:\n",
        "  ```cpp\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaError_t error = cudaGetLastError();\n",
        "  if (error != cudaSuccess) {\n",
        "      printf(\"CUDA hiba: %s\\n\", cudaGetErrorString(error));\n",
        "  }\n",
        "  ```\n",
        "\n",
        "4. Végül az [`cudaEventElapsedTime`](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html#group__CUDART__EVENT_1g40159125411db92c835edb46a0989cd6) függvény segítségével kiszámítjuk a két bélyeg között eltelt időt. Például:\n",
        "  ```cpp\n",
        "  float elapsedTime;\n",
        "  cudaEventElapsedTime(&elapsedTime, start, stop);\n",
        "  printf(\"A CUDA műveletek elvégzése %.2f ms telt.\\n\", elapsedTime);\n",
        "  ```\n",
        "5. A [`cudaEventDestroy`](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html#group__CUDART__EVENT_1g2cb6baa0830a1cd0bd957bfd8705045b) függvényt használjuk az általunk létrehozott `cudaEvent` objektumok felszabadítására. Például:\n",
        "  ```cpp\n",
        "  cudaEventDestroy(start);\n",
        "  cudaEventDestroy(stop);\n",
        "  ```\n",
        "\n",
        "Az alábbi példa összefoglalja a fenti lépéseket:\n",
        "\n",
        "```cpp\n",
        "__global__ void addKernel(int *c, const int *a, const int *b, int size)\n",
        "{\n",
        "    int i = threadIdx.x;\n",
        "    if (i < size) {\n",
        "        c[i] = a[i] + b[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    const int size = 1024;\n",
        "    int a[size], b[size], c[size];\n",
        "\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        a[i] = i;\n",
        "        b[i] = 2 * i;\n",
        "    }\n",
        "\n",
        "    int *dev_a, *dev_b, *dev_c;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    cudaEventRecord(start, 0);\n",
        "\n",
        "    cudaMalloc(&dev_a, size * sizeof(int));\n",
        "    cudaMalloc(&dev_b, size * sizeof(int));\n",
        "    cudaMalloc(&dev_c, size * sizeof(int));\n",
        "\n",
        "    cudaMemcpy(dev_a, a, size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    addKernel<<<1, size>>>(dev_c, dev_a, dev_b, size);\n",
        "\n",
        "    cudaMemcpy(c, dev_c, size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    \n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "    cudaFree(dev_c);\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaError_t error = cudaGetLastError();\n",
        "    if (error != cudaSuccess) {\n",
        "        printf(\"CUDA hiba: %s\\n\", cudaGetErrorString(error));\n",
        "    }\n",
        "    float elapsedTime;\n",
        "    cudaEventElapsedTime(&elapsedTime, start, stop);\n",
        "    printf(\"A CUDA művelet %.2f millió másodpercig tartott.\\n\", elapsedTime);\n",
        "    \n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "    return 0;\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "phM6n5Mt1Odq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Időmérés CPU-n\n",
        "\n",
        "C++-ban, egy nagyfelbontású aktuális időpontot, a\n",
        "```cpp\n",
        "static  std::chrono::time_point<std::chrono::high_resolution_clock> now()  noexcept;\n",
        "```\n",
        "[függvénnyel](https://en.cppreference.com/w/cpp/chrono/high_resolution_clock/now) kérhetjük le. A rendszer maximális időfelbontását (_ticks per second_) a `std::chrono::high_resolution_clock::period::den` érték lekérésével kaphatjuk meg.\n",
        "\n",
        "Ha kíváncsiak vagyunk egy kódrészlet futási idejére, megmérhetjük azt két időbélyegek segítségével, melyeket lekérdezünk mindjárt a kódrészlet előtt és után. A két időbélyeg különbsége megadja az eltelt időt, a kívánt [mértékegységben](https://en.cppreference.com/w/cpp/chrono/duration/duration_cast):\n",
        "\n",
        "![](https://i.ibb.co/F5D8brL/duration-cast.png)\n",
        "\n",
        "\n",
        "### Példa időmérésre:\n",
        "\n",
        "```cpp\n",
        "#include <iostream>\n",
        "#include <chrono>\n",
        "\n",
        "using  namespace std;\n",
        "\n",
        "int main()\n",
        "{\n",
        "\tcout  << chrono::high_resolution_clock::period::den  << endl;\n",
        "\tauto start_time = chrono::high_resolution_clock::now();\n",
        "\tlong temp =  0;\n",
        "\tfor  (auto i =  0; i < 100000000; i++)\n",
        "\t\ttemp += i;\n",
        "\tauto end_time = chrono::high_resolution_clock::now();\n",
        "\tcout  << chrono::duration_cast<chrono::seconds>(end_time - start_time).count()  <<  \":\";\n",
        "\tcout  << chrono::duration_cast<chrono::milliseconds>(end_time - start_time).count()  <<  \":\";\n",
        "\tcout  << chrono::duration_cast<chrono::microseconds>(end_time - start_time).count()  <<  \":\";\n",
        "\tcout << endl;\n",
        "\n",
        "\treturn  0;\n",
        "}\n",
        "```\n",
        "Mivel ez az időmérés a rendszerórát veszi figyelembe (wall clock time), és a számítások közben más folyamatok is ütemezésre kerülhettek, a pontos eredmény érdekében érdemes többször megmérni a futási időt, az kiugró értékeket eldobni, majd a többi mérésből átlagot számolni.\n",
        "\n",
        "A `clock_gettime` segítségével, POSIX kompatibilis rendszereken a `CLOCK_PROCESS_CPUTIME_ID` óra azonosító paraméter megadásával, folyamat szintű időmérést végezhetünk. A [https://en.cppreference.com/w/c/chrono/clock](https://en.cppreference.com/w/c/chrono/clock) található példa bemutatja a folyamat és rendszer idő közötti különbséget."
      ],
      "metadata": {
        "id": "9LtVX7QST6Sn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Véletlenszám generálás CUDA-ban\n",
        "\n",
        "A továbbiakban megismerkedünk a véletlen számok generálásának módszertanával CUDA-ban, a [`cuRAND`](https://docs.nvidia.com/cuda/curand/index.html)  könyvtár használatával.\n",
        "\n",
        "\n",
        "Egy álvéletlenszám sorozat generálása egy szekvenciális folyamat, ahol a következő számot az előbbi segítségével állítjuk elő. Például a [lineáris kongruenciális generátor](https://wikihuhu.top/wiki/Lehmer_random_number_generator) esetében\n",
        "\n",
        "\n",
        "```\n",
        "X_{n+1} = (aX_{n} + c) mod m\n",
        "```\n",
        "\n",
        "ahol ![](https://render.githubusercontent.com/render/math?math=$a$) a szorzó , ![](https://render.githubusercontent.com/render/math?math=$c$) a növekmény és ![](https://render.githubusercontent.com/render/math?math=$m$) a modulus.\n",
        "\n",
        "A kezdeti kifejezést magnak (angolul seed) nevezzük. Ez teszi lehetővé egy látszólag véletlenszerű szekvencia létrehozását. Minden maghoz új folytatást kapunk.\n",
        "\n",
        "Párhuzamosan generálni pszeudo-véletlen sorozattokat [kicsit bonyolultabb](https://quick-adviser.com/is-rand--thread-safe/\n",
        "), mivel egy naiv megközelítésben az `X_{n+1}` kiszámolásánál versenyhelyzet lép fel, több szál is ugyanazt  az értéket fogja generálni.\n",
        "\n",
        "\n",
        "Egy egyszerű megoldás, ha  minden szálhoz külön véletlenszerű állapotot hozunk létre, amelyet az egyes szálak a saját, a többiektől független véletlenszerű számsorozat létrehozásához használnak. Minden szál különböző magból, eredeti állapotból indul ki, annak érdekében, hogy mindegyikük egy különböző álváletlenszerű sorozatot generáljon.\n",
        "\n",
        "A [`cuRAND`](https://docs.nvidia.com/cuda/curand/index.html) könyvtár interfészt biztosít a véletlenszám-generátor állapotának inicializálásához szálanként, és ennek az állapotnak a felhasználásához véletlenszám-sorozatok generálásához. A következő lépések szükségesek ahhoz, hogy a cuRAND segítségével véletlenszerű számokat tudjunk generálni a CUDA programjainkban:\n",
        "\n",
        "1.\tA `cuRAND` fejlécek behívása.\n",
        "\n",
        "\t```cpp\n",
        "\t#include <curand_kernel.h>\n",
        "\t#include <curand.h>\n",
        "\t```\n",
        "2.\tMinden szálnak külön kell biztosítani egy eredeti magot,  különböző belső állapotot:\n",
        "\n",
        "\t-\tMemória lefoglalása a szálak állapváltozóinak (`curandState`).\n",
        "\n",
        "\t\t```cpp\n",
        "\t\tcurandState *dev_random;\n",
        "\t\tcudaMalloc((void**)&dev_random, num_threads_per_block*num_blocks*sizeof(curandState));\n",
        "\t\t```\n",
        "\n",
        "\t- Véletlenszerű állapotok inicializálása egy kernelben. Minden szál inicializálja a saját állapotát a GPU-n, a saját egyedi azonosítóját felhasználva:\n",
        "\n",
        "\t\t```cpp\n",
        "\t\t__global__ void gpu_random(..., curandState *states) {  \n",
        "\t\t\tint id = threadIdx.x + blockDim.x * blockIdx.x;  \n",
        "\n",
        "\t\t\t...  \n",
        "\t\t\tint seed = id; // different seed per thread         \n",
        "\t\t\tcurand_init(seed, id, 0, &states[id]);  //  Initialize CURAND        \n",
        "\t\t \t...\n",
        "\t\t ```\n",
        "\n",
        "3. Véletlen számok generálása a GPU-n valamilyen eloszlás szerint (pl. `curand_uniform`), szálanként a saját inicializált állapot használatával.\n",
        "\n",
        "\t```cpp\n",
        "\t__global__ void gpu_random(..., curandState *states) {  \n",
        "\t\tint id = threadIdx.x + blockDim.x * blockIdx.x;  float x;         \n",
        "\t\t...  \n",
        "\t\tcurand_init(seed, tid, 0, &states[id]);  //  Initialize CURAND         \n",
        "\t\t...\n",
        "\t\tfor(int i = 0; i < TRIALS_PER_THREAD; i++) {   \n",
        "\t\tx = curand_uniform (&states[id]);   \n",
        "\t\t...  \n",
        "\t}\n",
        "\t```\n",
        "\n",
        "\n",
        "A leggyakrabban használt eloszlások:\n",
        "\n",
        "```cpp\n",
        "// Egyenletes eloszlás\n",
        "__device__ float\n",
        "curand_uniform (curandState_t *state)\n",
        "\n",
        "// Normális eloszlás\n",
        "__device__ float\n",
        "curand_normal (curandState_t *state)\n",
        "\n",
        "// Log-normális eloszlás - a valószínűségi változó logaritmusa normális eloszlású.\n",
        "__device__ float\n",
        "curand_log_normal (curandState_t *state, float mean, float stddev)\n",
        "\n",
        "// Poisson-eloszlás\n",
        "__device__ unsigned int\n",
        "curand_poisson (curandState_t *state, double lambda)\n",
        "```\n",
        "\n",
        "Ezek a függvények elérhetőek dupla pontosságú lebegőpontos (`double`) számábrázolással is.\n",
        "\n",
        "## [Példa](https://github.com/deeperlearning/professional-cuda-c-programming/blob/master/examples/chapter08/rand-kernel.cu)\n",
        "\n",
        "```cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <curand_kernel.h>\n",
        "\n",
        "\n",
        "int threads_per_block = 256;\n",
        "int blocks_per_grid = 30;\n",
        "\n",
        "\n",
        "/*\n",
        " * device_api_kernel uses the cuRAND device API to generate random numbers\n",
        " * on-the-fly on the GPU, and then performs some dummy computation using them.\n",
        " */\n",
        "__global__ void device_api_kernel(curandState *states, float *out, int N)\n",
        "{\n",
        "    int i;\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int nthreads = gridDim.x * blockDim.x;\n",
        "    curandState *state = states + tid;\n",
        "\n",
        "    curand_init(9384, tid, 0, state);\n",
        "\n",
        "    for (i = tid; i < N; i += nthreads)\n",
        "    {\n",
        "        float rand = curand_uniform(state);\n",
        "        rand = rand * 2;\n",
        "        out[i] = rand;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "/*\n",
        " * use_device_api is an examples usage of the cuRAND device API to use the GPU\n",
        " * to generate random values on the fly from inside a CUDA kernel.\n",
        " */\n",
        "void use_device_api(int N)\n",
        "{\n",
        "    int i;\n",
        "    static curandState *states = NULL;\n",
        "    float *dOut, *hOut;\n",
        "\n",
        "    /*\n",
        "     * Allocate device memory to store the output and cuRAND device state\n",
        "     * objects (which are analogous to handles, but on the GPU).\n",
        "     */\n",
        "    cudaMalloc((void **)&dOut, sizeof(float) * N);\n",
        "    cudaMalloc((void **)&states, sizeof(curandState) *\n",
        "                threads_per_block * blocks_per_grid);\n",
        "    hOut = (float *)malloc(sizeof(float) * N);\n",
        "\n",
        "    // Execute a kernel that generates and consumes its own random numbers\n",
        "    device_api_kernel<<<blocks_per_grid, threads_per_block>>>(states, dOut, N);\n",
        "\n",
        "    // Retrieve the results\n",
        "    cudaMemcpy(hOut, dOut, sizeof(float) * N, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"Sampling of output from device API:\\n\");\n",
        "\n",
        "    for (i = 0; i < 10; i++)\n",
        "    {\n",
        "        printf(\"%2.4f\\n\", hOut[i]);\n",
        "    }\n",
        "\n",
        "    printf(\"...\\n\");\n",
        "\n",
        "    free(hOut);\n",
        "    cudaFree(dOut);\n",
        "    cudaFree(states);\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    int N = 8388608;\n",
        "\n",
        "    use_device_api(N);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "zwNDgRyiALmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CUDA profilozása az NVIDIA Profiler segítségével\n",
        "\n",
        "A CUDA-programok hatékonyságának elemzéséhez gyakran nem elég csupán a kernel futási időt mérni, mivel ez nem ad részletes információt a **memóriahozzáférésekről, regiszterhasználatról, párhuzamos végrehajtás hatékonyságáról és az adatmozgatásokról**. Az [NVIDIA Profiler eszközei](https://developer.nvidia.com/performance-analysis-tools), például **nvprof, Nsight Systems és Nsight Compute**, lehetőséget adnak mélyebb teljesítményprofilozásra.  \n",
        "\n",
        "**Mivel más vagy több  az NVIDIA Profiler, mint egy egyszerű időmérés?**  \n",
        "- Megmutatja, hogy a kernel futásán belül hol vannak szűk keresztmetszetek.  \n",
        "- Elemzi a **memóriahasználatot** (globális, megosztott memória, L2 cache kihasználtság).  \n",
        "- Vizsgálja a **számítási kihasználtságot** (SM-kihasználtság, párhuzamos végrehajtás).  \n",
        "- Részletes jelentést nyújt az egyes CUDA API hívásokról és memóriamásolásokról.  \n",
        "\n",
        "Az [nvprof](https://docs.csc.fi/computing/nvprof/) eszköz egy parancssori profiler, nem igényel GUI-t, ezért Google Colab környezetben is könnyen tudjuk használni.\n",
        "\n",
        "\n",
        "Az nvprof-nak csak egyszerűen át kell adnunk a CUDA programunkat mint paraméter:\n",
        "\n",
        "```bash\n",
        "nvprof ./cuda_program\n",
        "```\n",
        "\n",
        "Ez majd automatikusan rögzíti az összes CUDA API hívást, kernel futtatást és memóriaátvitelt, majd egy összegző jelentést készít.\n",
        "\n",
        "---\n",
        "\n",
        "## Példa -  CUDA program profilozása\n",
        "Vizsgáljuk meg egy egyszerű CUDA-kernel futását és memóriahasználatát az nvprof segítségével.\n"
      ],
      "metadata": {
        "id": "gLqgmC6Hc6TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile profiletest.cu\n",
        "\n",
        "#include <cuda.h>\n",
        "#include <curand_kernel.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void vector_add(float *a, float *b, float *c, int n, unsigned long long seed) {\n",
        "    int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // Véletlenszám-generátor inicializálása minden szálnál\n",
        "    curandState state;\n",
        "    curand_init(seed, i, 0, &state);\n",
        "\n",
        "    if (i < n) {\n",
        "        float noise = curand_uniform(&state) * 0.1f - 0.05f; // Zaj [-0.05, 0.05] intervallumban\n",
        "        c[i] = a[i] + b[i] + noise;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 1000000;\n",
        "    size_t size = n * sizeof(float);\n",
        "\n",
        "    // Memória foglalás a hoston\n",
        "    float *h_a = (float*)malloc(size);\n",
        "    float *h_b = (float*)malloc(size);\n",
        "    float *h_c = (float*)malloc(size);\n",
        "\n",
        "    // Inicializálás\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        h_a[i] = 1.0f;\n",
        "        h_b[i] = 2.0f;\n",
        "    }\n",
        "\n",
        "    // Memória foglalás a GPU-n\n",
        "    float *d_a, *d_b, *d_c;\n",
        "    cudaMalloc(&d_a, size);\n",
        "    cudaMalloc(&d_b, size);\n",
        "    cudaMalloc(&d_c, size);\n",
        "\n",
        "    // Adatok másolása a GPU-ra\n",
        "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Kernel indítása\n",
        "    int blockSize = 256;\n",
        "    int numBlocks = (n + blockSize - 1) / blockSize;\n",
        "    vector_add<<<numBlocks, blockSize>>>(d_a, d_b, d_c, n, 1234ULL);\n",
        "\n",
        "    // Eredmények visszamásolása\n",
        "    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Memória felszabadítás\n",
        "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);\n",
        "    free(h_a); free(h_b); free(h_c);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jDQXyDIenbP",
        "outputId": "647aa199-84e2-455e-8792-d3c5eed89f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing profiletest.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -o vector_add_noise profiletest.cu"
      ],
      "metadata": {
        "id": "BvnpAwVGfw9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./vector_add_noise"
      ],
      "metadata": {
        "id": "RLvoi-rQFChZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMb0LF4-gbVB",
        "outputId": "61851695-3c07-4e47-fad6-7ae2308bb9ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvprof: NVIDIA (R) Cuda command line profiler\n",
            "Copyright (c) 2012 - 2024 NVIDIA Corporation\n",
            "Release version 12.5.82 (21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luX63OvbhCz_",
        "outputId": "354f84fa-9a71-4766-dacf-a75be78155c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar 25 15:40:05 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./vector_add_noise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8Yd2C43etK7",
        "outputId": "0f1ab201-f5da-4a05-b950-e8225a052918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==12986== NVPROF is profiling process 12986, command: ./vector_add_noise\n",
            "==12986== Profiling application: ./vector_add_noise\n",
            "==12986== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   93.16%  41.018ms         1  41.018ms  41.018ms  41.018ms  vector_add(float*, float*, float*, int, __int64)\n",
            "                    3.72%  1.6385ms         1  1.6385ms  1.6385ms  1.6385ms  [CUDA memcpy DtoH]\n",
            "                    3.12%  1.3719ms         2  685.95us  684.72us  687.19us  [CUDA memcpy HtoD]\n",
            "      API calls:   71.18%  119.69ms         3  39.897ms  64.851us  119.55ms  cudaMalloc\n",
            "                   27.52%  46.274ms         3  15.425ms  906.25us  43.921ms  cudaMemcpy\n",
            "                    0.60%  1.0016ms         1  1.0016ms  1.0016ms  1.0016ms  cuDeviceGetPCIBusId\n",
            "                    0.32%  542.03us         3  180.68us  127.45us  209.25us  cudaFree\n",
            "                    0.29%  484.67us         1  484.67us  484.67us  484.67us  cudaLaunchKernel\n",
            "                    0.08%  139.67us       114  1.2250us     105ns  54.284us  cuDeviceGetAttribute\n",
            "                    0.01%  11.514us         1  11.514us  11.514us  11.514us  cuDeviceGetName\n",
            "                    0.00%  1.6920us         3     564ns     152ns  1.2830us  cuDeviceGetCount\n",
            "                    0.00%     996ns         2     498ns     172ns     824ns  cuDeviceGet\n",
            "                    0.00%     474ns         1     474ns     474ns     474ns  cuDeviceTotalMem\n",
            "                    0.00%     408ns         1     408ns     408ns     408ns  cuModuleGetLoadingMode\n",
            "                    0.00%     274ns         1     274ns     274ns     274ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mit tudunk meg ebből?**\n",
        "- A **vector_add kernel** a teljes futási idő **92.15%-át** teszi ki.\n",
        "- Az **adatmásolás** (Host → Device és Device → Host) mennyi időt vesz igénybe (~4.47% + 3.38%).\n",
        "- Milyen API hívások hajtodtak végre.\n",
        "\n",
        "\n",
        "## Részletesebb Elemzés\n",
        "Az nvprof további hasznos lehetőségeket biztosít. A GPU-műveletek részletes megjelenítése hasnzáljuk:\n"
      ],
      "metadata": {
        "id": "OeAUXUuVetmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof --print-gpu-trace ./vector_add_noise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZliB41lQjPTr",
        "outputId": "0319de55-e843-4fd7-a338-67bbd6ded392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==13065== NVPROF is profiling process 13065, command: ./vector_add_noise\n",
            "==13065== Profiling application: ./vector_add_noise\n",
            "==13065== Profiling result:\n",
            "   Start  Duration            Grid Size      Block Size     Regs*    SSMem*    DSMem*      Size  Throughput  SrcMemType  DstMemType           Device   Context    Stream  Name\n",
            "167.49ms  759.38us                    -               -         -         -         -  3.8147MB  4.9057GB/s    Pageable      Device     Tesla T4 (0)         1         7  [CUDA memcpy HtoD]\n",
            "168.49ms  746.42us                    -               -         -         -         -  3.8147MB  4.9909GB/s    Pageable      Device     Tesla T4 (0)         1         7  [CUDA memcpy HtoD]\n",
            "169.82ms  41.030ms           (3907 1 1)       (256 1 1)        61        0B        0B         -           -           -           -     Tesla T4 (0)         1         7  vector_add(float*, float*, float*, int, __int64) [130]\n",
            "210.85ms  1.9315ms                    -               -         -         -         -  3.8147MB  1.9287GB/s      Device    Pageable     Tesla T4 (0)         1         7  [CUDA memcpy DtoH]\n",
            "\n",
            "Regs: Number of registers used per CUDA thread. This number includes registers used internally by the CUDA driver and/or tools and can be more than what the compiler shows.\n",
            "SSMem: Static shared memory allocated per CUDA block.\n",
            "DSMem: Dynamic shared memory allocated per CUDA block.\n",
            "SrcMemType: The type of source memory accessed by memory operation/copy\n",
            "DstMemType: The type of destination memory accessed by memory operation/copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "rSTDuO6W9mRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```bash\n",
        "nvprof --print-gpu-trace ./vector_add\n",
        "```\n",
        "\n",
        "A profilozás az alkalmazás futtatásának különböző aspektusait méri, mint például a memória másolásokat, kernel futtatásokat, és a GPU erőforrások használatát. A profilozás minden CUDA művelet pontos időbélyegét megmutatja. Pontosan mit láthatunk?\n",
        "\n",
        "\n",
        "### Értelmezés\n",
        "\n",
        "1. CUDA Memcpy HtoD (Host-to-Device Memory Copy)\n",
        "- **`243.29ms`** és **`244.22ms`**: Ez az időpont, amikor a memória másolás elkezdődik és befejeződik. A másolás a host (CPU) és a device (GPU) között történik.\n",
        "- **`696.81us` és `707.98us`**: A memória másolás (Host to Device) ideje, tehát mennyi időbe telt az adatokat átmásolni a CPU-ról a GPU-ra.\n",
        "- **`3.8147MB`**: A memória másolás mérete. Ez azt jelenti, hogy az adatok mérete körülbelül 3.8 MB volt, amit átvittünk.\n",
        "- **`5.3462GB/s` és `5.2619GB/s`**: A memória másolás sebessége, azaz hány GB adatot másoltunk másodpercenként.\n",
        "\n",
        "A **`SrcMemType`** és **`DstMemType`** az adatok forrás- és célmemóriájának típusát jelzi.\n",
        "- **`Pageable`**: Ez azt jelenti, hogy a memóriát nem tartja fixen a rendszer, így a memória elérhetősége rugalmas.\n",
        "- **`Device`**: A célmemória a GPU memóriája.\n",
        "\n",
        "\n",
        "2. **CUDA Memcpy DtoH (Device-to-Host Memory Copy)**\n",
        "- **`286.35ms`** és **`1.6342ms`**: A másolás befejeződése és az időtartam. Ez az adat visszamásolását jelenti a GPU-ról a CPU-ra.\n",
        "- **`3.8147MB`**: A visszamásolt adat mennyisége.\n",
        "- **`2.2796GB/s`**: Az adatok átvitelének sebessége a GPU-ról a CPU-ra.\n",
        "\n",
        "\n",
        "3. **Kernel futás: `vector_add`**\n",
        "- **`245.41ms`**: A kernel végrehajtásának időtartama, ami a `vector_add` nevű függvény futtatása.\n",
        "- **`40.934ms`**: Ez az idő, amely alatt a kernel ténylegesen végrehajtódott.\n",
        "- **`(3907 1 1)`**: A **grid mérete** (háromdimenziós), ami azt jelzi, hogy hány blokkot futtatott a kernel: 3907 blokkot.\n",
        "- **`(256 1 1)`**: A **block mérete** (háromdimenziós), ami azt jelzi, hogy hány szálat futtatott egy blokkban: 256 szál.\n",
        "- **`61`**: A kernel által használt **regiszterek** száma szálanként. A regiszterek a CPU/GPU gyors memóriája, amely a szálak gyors adatkezeléséhez szükségesek.\n",
        "- **`0B`**: **Static shared memory (SSMem)** és **Dynamic shared memory (DSMem)**: Ez a memória, amelyet a blokk szálai osztanak meg egymással. Most nem lett használva sem statikus, sem dinamikus memória.\n",
        "  \n",
        "- A kernel neve: **`vector_add(float*, float*, float*, int, __int64)`**, ami azt jelzi, hogy egy vektorműveletet hajtott végre, ahol három lebegőpontos számokból álló vektort adtak össze.\n",
        "\n",
        "\n",
        "\n",
        "### Következtetés\n",
        "\n",
        "- Az adatátvitel (Host-to-Device és Device-to-Host) viszonylag gyors volt, 5-5 GB/s sebességgel.\n",
        "- A **`vector_add`** kernel futása 245.41ms-ig tartott, és az átlagos végrehajtási idő 40.934ms volt. Az adatokat 3907 blokkban és 256 szálat használva dolgozták fel.\n",
        "- A regiszterek száma szálanként 61, ami azt jelzi, hogy nem használtak túl sok regisztert a kernelben. A kernel nem használt megosztott memóriát (sem statikus, sem dinamikus).\n",
        "\n",
        "Mint látjuk, a profilozás segít abban, hogy áttekinthessük a program GPU erőforrás-használatát, és láthatjuk, hogy a memória másolás és a kernel végrehajtás hogyan befolyásolja a program teljesítményét.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OgczpNuQjPi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memóriahasználat Profilozása"
      ],
      "metadata": {
        "id": "rFQAeDfIk--F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof --print-gpu-summary ./vector_add_noise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VYub3JOk_Q_",
        "outputId": "2b67dd1e-a7b3-4d4d-c97f-96d6900ad098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==4856== NVPROF is profiling process 4856, command: ./vector_add_noise\n",
            "==4856== Profiling application: ./vector_add_noise\n",
            "==4856== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   92.87%  40.918ms         1  40.918ms  40.918ms  40.918ms  vector_add(float*, float*, float*, int, __int64)\n",
            "                    3.68%  1.6196ms         1  1.6196ms  1.6196ms  1.6196ms  [CUDA memcpy DtoH]\n",
            "                    3.45%  1.5220ms         2  761.00us  743.66us  778.35us  [CUDA memcpy HtoD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Az **`nvprof --print-gpu-summary`** parancs összegző profilozási adatokat adott vissza a CUDA programról, beleértve a GPU aktivitások időarányos megoszlását, valamint a különböző kernel-ek és memória műveletek végrehajtásának időtartamát.\n",
        "\n",
        "A következő adatokat láthatjuk az eredményben:\n",
        "\n",
        "1. GPU aktivitások:\n",
        "- **`92.87%`**: A **GPU aktivitások összes idejének 92.87%-át** a `vector_add` kernel végrehajtása tette ki. Ez azt jelzi, hogy a program fő részét a kernel futtatása képezte.\n",
        "  - **`40.918ms`**: A `vector_add` kernel teljes futási ideje.\n",
        "  - **`1`**: A kernel pontosan **1 alkalommal** futott.\n",
        "  - **`40.918ms`**: Az egyes futások időtartama, ami azonos minden egyes kernel végrehajtás esetén (mivel csak egyszer futott).\n",
        "\n",
        "2. **[CUDA memcpy DtoH] (Device-to-Host Memory Copy):\n",
        "- **`3.68%`**: A memória másolás a GPU-ról a CPU-ra (Device-to-Host) **3.68%-ban** tette ki a teljes időt.\n",
        "  - **`1.6196ms`**: Ez a másolás teljes időtartama.\n",
        "  - **`1`**: A memória másolás pontosan **1 alkalommal** történt.\n",
        "  - **`1.6196ms`**: Az egyes memória másolások időtartama (mivel csak egyszer történt másolás).\n",
        "\n",
        "3. **[CUDA memcpy HtoD] (Host-to-Device Memory Copy):\n",
        "- **`3.45%`**: A memória másolás a CPU-ról a GPU-ra (Host-to-Device) **3.45%-ban** tette ki a teljes időt.\n",
        "  - **`1.5220ms`**: A memória másolás teljes időtartama (összesen két másolás történt).\n",
        "  - **`2`**: A memória másolás **2 alkalommal** történt.\n",
        "  - **`761.00us`**: Az egyes másolások átlagos időtartama (768us és 743us között ingadozott).\n",
        "  - **`743.66us`** és **`778.35us`**: A másolások közötti időeltérés, tehát az időintervallumok minimális és maximális értékei.\n",
        "\n",
        "Összegzés:\n",
        "- **A `vector_add` kernel** a legnagyobb részét tette ki a program futásának, és **40.918 ms**-ot igényelt.\n",
        "- **Memória másolás**:\n",
        "  - A **GPU-ról a CPU-ra történő másolás** (`[CUDA memcpy DtoH]`) körülbelül **1.6196 ms**-ot vett igénybe.\n",
        "  - A **CPU-ról a GPU-ra történő másolás** (`[CUDA memcpy HtoD]`) összesen **1.5220 ms** időt vett igénybe, amelyet két különböző másolás hajtott végre.\n",
        "\n",
        "Itt is látjuk, a legnagyobb időt a **kernel végrehajtása** tette ki, míg a memória másolás (GPU és CPU között) viszonylag kis időt vett igénybe az összes futási időhöz képest.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Általános optimalizálási tippek\n",
        "- Memóriahasználat csökkentése: Ha a memória másolás időigényes, próbáljunk meg kevesebb adatot mozgatni.  \n",
        "- Regiszterhasználat figyelése: Ha túl sok a regiszter, kevesebb thread tud futni párhuzamosan.  \n",
        "- Optimalizált grid/block méretek: Ha a kihasználtság alacsony, próbáljunk meg más **blockSize** értékeket.  \n",
        "- Ha az L2 cache hit rate alacsony, érdemes **megosztott memóriát (shared memory)** használni a teljesítmény növelése érdekében.\n",
        "\n",
        "\n",
        "## nvprof összegzés\n",
        "| **Parancs** | **Mit csinál?** |\n",
        "|-------------|----------------|\n",
        "| `nvprof ./program` | Alapvető profilozás, futási idők elemzése |\n",
        "| `nvprof --print-gpu-trace ./program` | Részletes GPU műveletek megjelenítése |\n",
        "| `nvprof --print-gpu-summary ./program` | Memóriahasználat és kihasználtság elemzése |\n",
        "\n",
        "Az nvprof egy egyszerű, de erőteljes parancssori eszköz a CUDA-programok teljesítményének mérésére. Ha mélyebb elemzést szeretnénk, az **Nsight Systems és Nsight Compute** további részleteket adhat.\n"
      ],
      "metadata": {
        "id": "duSCPZCmk_cG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feladatok\n",
        "\n",
        "1. Írjunk egy CUDA programot, amely véletlenszerű pontokat generál az egységnégyzetben \\([0,1] \\times [0,1]\\), majd kiszámolja a pontok távolságát az origótól.  \n",
        "  Lépések:\n",
        "  - Minden CUDA szál generáljon egy véletlen \\( (x, y) \\) pontot a \\([0,1]\\) tartományban.  \n",
        "  - Számoljuk ki a pontok távolságát az origótól:  \n",
        "   $d = \\sqrt{x^2 + y^2}$\n",
        "  - Tároljuk az eredményeket egy tömbben.  \n",
        "  - Másoljuk vissza az adatokat a CPU-ra, és számoljuk ki az átlagos távolságot.  \n",
        "\n",
        "  Az átlagos távolságnak közel kell lennie az elméleti értékhez:  $E[d] \\approx 0.521405$\n",
        "\n",
        "2. Írjunk egy szekvenciális majd egy CUDA programot mely Monte Carlo módszer segítségével, ![](https://render.githubusercontent.com/render/math?math=$n$) véletlenszerű pontot generálva, megközelítően kiszámítja a ![](https://render.githubusercontent.com/render/math?math=$\\pi$) értékét. Az ![](https://render.githubusercontent.com/render/math?math=$n$) a program paramétere.\n",
        "\n",
        "  A módszer leírása:\n",
        "    \n",
        "  - [http://nagysandor.eu/physlet/applets/iter1.html](http://nagysandor.eu/physlet/applets/iter1.html)\n",
        "\n",
        "  - [http://www.tldp.org/HOWTO/Parallel-Processing-HOWTO-2.html](http://www.tldp.org/HOWTO/Parallel-Processing-HOWTO-2.html)\n",
        "\n",
        "3. A tanult CUDA időmérést felhasználva számoljuk ki CUDA-programjaink gyorsulását. Ehhez természetesen szükség van a szekvenciális verziókra is, amelyek a hoston futnak.  \n",
        "\n",
        "  A hoston az időmérést a `chrono::high_resolution_clock` segítségével végezzük, míg a CUDA-kernélek esetében a `cudaEventCreate` és `cudaEventRecord` függvényeket használjuk.\n",
        "\n",
        "4. Profilozzuk a megírt programokat. Vizsgáljuk meg a kernelek futási idejét, a regiszterek és memóriahasználatot, majd az észrevételeket egy rövid beszámolóban rögzítsük (ez lehet külön fájl vagy egy új \"text\" cella a notebook-ban).\n"
      ],
      "metadata": {
        "id": "OHJTyfOOHyjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile first_exercise.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <curand_kernel.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <math.h>\n",
        "#include <time.h>\n",
        "\n",
        "// Kernel: minden szál generál egy véletlen pontot és kiszámolja a távolságot az origótól.\n",
        "__global__ void kernel(float *distances, unsigned int seed, int n) {\n",
        "    int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        // Inicializáljuk a curand állapotot minden szálban\n",
        "        curandState state;\n",
        "        curand_init(seed, idx, 0, &state);\n",
        "\n",
        "        // Véletlen számok [0,1]-ben\n",
        "        float x = curand_uniform(&state);\n",
        "        float y = curand_uniform(&state);\n",
        "\n",
        "        // Távolság az origótól: sqrt(x*x + y*y)\n",
        "        float d = sqrtf(x * x + y * y);\n",
        "        distances[idx] = d;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int n = 1000000; // Pontok száma\n",
        "    size_t size = n * sizeof(float);\n",
        "\n",
        "    // Host memóriában tárolt eredmény tömb\n",
        "    float *h_distances = (float *) malloc(size);\n",
        "\n",
        "    // Device memóriára foglalás\n",
        "    float *d_distances;\n",
        "    cudaMalloc((void **) &d_distances, size);\n",
        "\n",
        "    // Kernel konfiguráció: 256 szál blokkanként\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocks = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "    // Kernel indítása; a seed értéke a rendszeridő alapján kerül beállításra\n",
        "    kernel<<<blocks, threadsPerBlock>>>(d_distances, time(NULL), n);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Adatok átvitele a GPU-ról a CPU-ra\n",
        "    cudaMemcpy(h_distances, d_distances, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Átlagos távolság kiszámolása a CPU-n\n",
        "    double sum = 0.0;\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        sum += h_distances[i];\n",
        "    }\n",
        "    double avg = sum / n;\n",
        "    printf(\"Átlagos távolság: %f\\n\", avg);\n",
        "\n",
        "    // Memória felszabadítása\n",
        "    cudaFree(d_distances);\n",
        "    free(h_distances);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "PJLNtmjbn2N5",
        "outputId": "13b67d16-76fb-444a-d94f-3a17d3517dab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting first_exercise.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -o first_exercise_out first_exercise.cu"
      ],
      "metadata": {
        "id": "aehwfKI5-DZI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./first_exercise_out"
      ],
      "metadata": {
        "id": "2ydGNlFB_H68",
        "outputId": "944423a2-51bd-420d-843b-04c34abf990a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Átlagos távolság: 0.765073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile second_exercise.cu\n",
        "\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <time.h>\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    if (argc != 2) {\n",
        "        printf(\"Használat: %s pontok_száma\\n\", argv[0]);\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    long long num_points = atoll(argv[1]);\n",
        "    long long count_inside = 0;\n",
        "\n",
        "    // Véletlen generátor inicializálása\n",
        "    srand(time(NULL));\n",
        "\n",
        "    // Monte Carlo: generáljunk pontokat az [0,1]x[0,1] egységnégyzetben\n",
        "    for (long long i = 0; i < num_points; i++) {\n",
        "        double x = (double)rand() / RAND_MAX;\n",
        "        double y = (double)rand() / RAND_MAX;\n",
        "        // Ha a pont az egységsugarú körön belül van\n",
        "        if (x*x + y*y <= 1.0) {\n",
        "            count_inside++;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // π közelítése: a kör területe ¼ * π, így π ≈ 4 * (belső pontok száma) / (összes pontok száma)\n",
        "    double pi = 4.0 * count_inside / num_points;\n",
        "    printf(\"Közelített π érték: %f\\n\", pi);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "Cy6M2Ab4l7H7",
        "outputId": "a87e78d0-645b-419a-c737-60e9a7414f17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting second_exercise.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -o second_exercise_out second_exercise.cu"
      ],
      "metadata": {
        "id": "SL8RngXKmGfD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./second_exercise_out 10000000"
      ],
      "metadata": {
        "id": "nJoXFZErmH--",
        "outputId": "cf93e15b-8485-44fa-96dd-8dc217ba71be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Közelített π érték: 3.141109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile second_exercise.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <curand_kernel.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <time.h>\n",
        "\n",
        "// CUDA kernel, mely minden szál a saját részfeladatát végzi.\n",
        "// Az eredmény (a kör belsejében lévő pontok száma) globális változóba kerül atomikusan összegzésre.\n",
        "__global__ void monteCarloPi(unsigned long long *count, int num_points, unsigned int seed) {\n",
        "    int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    unsigned long long local_count = 0;\n",
        "    curandState state;\n",
        "\n",
        "    // Minden szál inicializálja a saját curand állapotát\n",
        "    curand_init(seed, idx, 0, &state);\n",
        "\n",
        "    // Grid-stride ciklussal több iterációt is lefutattunk, ha a szálak száma kevesebb, mint a pontok száma.\n",
        "    for (int i = idx; i < num_points; i += blockDim.x * gridDim.x) {\n",
        "        float x = curand_uniform(&state);\n",
        "        float y = curand_uniform(&state);\n",
        "        if (x * x + y * y <= 1.0f) {\n",
        "            local_count++;\n",
        "        }\n",
        "    }\n",
        "    // Az atomikus összeadás segítségével összegezzük az egyes szálak eredményét.\n",
        "    atomicAdd(count, local_count);\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    if (argc != 2) {\n",
        "        printf(\"Használat: %s pontok_száma\\n\", argv[0]);\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    int num_points = atoi(argv[1]);\n",
        "    unsigned long long h_count = 0;\n",
        "    unsigned long long *d_count;\n",
        "\n",
        "    // Memória foglalása a GPU-n a pontok számának összegzésére\n",
        "    cudaMalloc((void**)&d_count, sizeof(unsigned long long));\n",
        "    cudaMemcpy(d_count, &h_count, sizeof(unsigned long long), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // CUDA kernel futtatása: blokkonként 256 szál, a blokkok számát úgy számoljuk, hogy lefedjük az összes pontot\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocks = (num_points + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "    // Véletlen seed a rendszeridő alapján\n",
        "    unsigned int seed = time(NULL);\n",
        "    monteCarloPi<<<blocks, threadsPerBlock>>>(d_count, num_points, seed);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Az eredmény átmásolása a CPU memóriába\n",
        "    cudaMemcpy(&h_count, d_count, sizeof(unsigned long long), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // π közelítése: 4 * (belső pontok száma) / (összes pontok száma)\n",
        "    double pi = 4.0 * h_count / num_points;\n",
        "    printf(\"Közelített π érték: %f\\n\", pi);\n",
        "\n",
        "    // GPU memória felszabadítása\n",
        "    cudaFree(d_count);\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "w9RvYZmamXwM",
        "outputId": "b1b7911a-a328-4644-fd7b-abd3d377be37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting second_exercise.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -o second_exercise_out second_exercise.cu"
      ],
      "metadata": {
        "id": "0_TSikqEmd3z"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./second_exercise_out 10000000"
      ],
      "metadata": {
        "id": "WZkiQKtqmffb",
        "outputId": "f8e55a79-2bfd-4238-82f9-fc8fb6fdd089",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Közelített π érték: 3.141168\n"
          ]
        }
      ]
    }
  ]
}